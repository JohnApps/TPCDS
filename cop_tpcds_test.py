# cop_tpcds_test.py
# Generated by CoPilot
#
# run_tpcds_duckdb_with_combined_plot.py
# Executes TPC-DS queries (1..99) against tpcds_local.db, times them, collects per-query metadata,
# collects table row counts, produces separate plots and a combined plot with two y-axes.
# Query results are not printed or exported.
# run_tpcds_duckdb_with_system_metrics.py
# Executes TPC-DS queries (1..99) against tpcds_local.db, profiles runtime and result row count,
# and captures CPU, peak memory (process RSS), and disk I/O (read/write bytes) per query.
# Query results are not printed or exported.

import duckdb
import pandas as pd
import matplotlib.pyplot as plt
import time
import re
import threading
from pathlib import Path
import psutil
import os

# CONFIG
DB_PATH = "tpcds_local.db"
QUERIES_DIR = Path("tpcds_queries")
QUERY_FILENAME_PATTERN = re.compile(r"q?0*([1-9][0-9]?)\.sql$", re.IGNORECASE)
OUTPUT_DIR = Path("tpcds_results")
OUTPUT_DIR.mkdir(exist_ok=True)
SAMPLE_INTERVAL = 0.05  # seconds for background sampling during query execution

# Load queries from directory
def load_queries_from_dir(dirpath):
    queries = {}
    if not dirpath.exists():
        return queries
    for p in sorted(dirpath.glob("*.sql")):
        m = QUERY_FILENAME_PATTERN.search(p.name)
        if not m:
            continue
        qnum = int(m.group(1))
        sql = p.read_text(encoding="utf8")
        queries[qnum] = sql
    return queries

# Monitor helper: background sampler to record process RSS peaks and optionally sample system metrics
class ResourceMonitor:
    def __init__(self, interval=SAMPLE_INTERVAL):
        self.interval = interval
        self._stop_event = threading.Event()
        self._thread = None
        self.proc = psutil.Process(os.getpid())
        self.num_cpus = psutil.cpu_count(logical=True) or 1

        # initial baseline snapshots
        self.disk_before = psutil.disk_io_counters()  # system-wide
        self.cpu_times_before = self.proc.cpu_times()
        self.start_time = None

        # samples collected
        self.rss_samples = []
        self.timestamps = []
        self.disk_samples = []  # (read_bytes, write_bytes)
        self._lock = threading.Lock()

    def _sample_loop(self):
        while not self._stop_event.is_set():
            with self._lock:
                ts = time.time()
                try:
                    rss = self.proc.memory_info().rss
                except Exception:
                    rss = 0
                try:
                    dio = psutil.disk_io_counters()
                    read_b = getattr(dio, "read_bytes", 0)
                    write_b = getattr(dio, "write_bytes", 0)
                except Exception:
                    read_b = write_b = 0
                self.rss_samples.append(rss)
                self.disk_samples.append((read_b, write_b))
                self.timestamps.append(ts)
            time.sleep(self.interval)

    def start(self):
        self.start_time = time.time()
        self.disk_before = psutil.disk_io_counters()
        self.cpu_times_before = self.proc.cpu_times()
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._sample_loop, daemon=True)
        self._thread.start()

    def stop(self):
        # stop sampling and compute metrics
        self._stop_event.set()
        if self._thread is not None:
            self._thread.join(timeout=1.0)
        self.end_time = time.time()
        self.disk_after = psutil.disk_io_counters()
        self.cpu_times_after = self.proc.cpu_times()

    def get_metrics(self):
        wall_time = max(1e-9, (self.end_time - self.start_time))
        # process CPU time delta (user + system)
        cpu_delta = ((self.cpu_times_after.user + self.cpu_times_after.system) -
                     (self.cpu_times_before.user + self.cpu_times_before.system))
        # CPU utilization percent relative to a single CPU, normalized to number of CPUs
        cpu_util_percent = (cpu_delta / wall_time) * 100.0 / (self.num_cpus or 1)

        # peak RSS in MB
        peak_rss = 0
        with self._lock:
            if self.rss_samples:
                peak_rss = max(self.rss_samples)
        peak_rss_mb = peak_rss / (1024.0 * 1024.0)

        # disk io deltas (system-wide)
        read_bytes = max(0, (getattr(self.disk_after, "read_bytes", 0) - getattr(self.disk_before, "read_bytes", 0)))
        write_bytes = max(0, (getattr(self.disk_after, "write_bytes", 0) - getattr(self.disk_before, "write_bytes", 0)))

        return {
            "cpu_percent": cpu_util_percent,
            "peak_rss_mb": peak_rss_mb,
            "disk_read_bytes": read_bytes,
            "disk_write_bytes": write_bytes,
            "wall_time": wall_time
        }

# DB helpers
def list_tables(connection):
    q = """
    SELECT table_schema, table_name
    FROM information_schema.tables
    WHERE table_type = 'BASE TABLE'
      AND table_schema NOT IN ('pg_catalog', 'information_schema')
    ORDER BY table_schema, table_name
    """
    df = connection.execute(q).fetchdf()
    full_names = []
    for _, row in df.iterrows():
        schema, tbl = row["table_schema"], row["table_name"]
        if schema and schema != "main":
            full_names.append(f"{schema}.{tbl}")
        else:
            full_names.append(tbl)
    return full_names

def get_table_row_counts(connection, tables):
    table_counts = []
    for tbl in tables:
        try:
            cnt = connection.execute(f"SELECT COUNT(*) AS c FROM {tbl}").fetchone()[0]
        except Exception as e:
            cnt = None
            print(f"Warning: counting rows for table {tbl} failed: {e}")
        table_counts.append({"table": tbl, "row_count": cnt})
    return pd.DataFrame(table_counts)

# Execute queries and profile runtime + resources
def execute_and_profile_queries(connection, queries_dict):
    records = []
    proc = psutil.Process(os.getpid())
    for qnum in range(1, 100):
        if qnum not in queries_dict:
            records.append({
                "query_id": qnum,
                "status": "missing",
                "runtime_seconds": None,
                "result_row_count": None,
                "cpu_percent": None,
                "peak_rss_mb": None,
                "disk_read_bytes": None,
                "disk_write_bytes": None,
                "error": "SQL file missing"
            })
            continue

        sql = queries_dict[qnum]
        monitor = ResourceMonitor(interval=SAMPLE_INTERVAL)
        try:
            monitor.start()
            t0 = time.time()
            # Execute query and fetch into DataFrame (do not print or store results)
            df = connection.execute(sql).fetchdf()
            runtime = time.time() - t0
            rows = int(df.shape[0])
            # free dataframe
            del df
            monitor.stop()
            metrics = monitor.get_metrics()
            records.append({
                "query_id": qnum,
                "status": "ok",
                "runtime_seconds": runtime,
                "result_row_count": rows,
                "cpu_percent": metrics["cpu_percent"],
                "peak_rss_mb": metrics["peak_rss_mb"],
                "disk_read_bytes": metrics["disk_read_bytes"],
                "disk_write_bytes": metrics["disk_write_bytes"],
                "error": None
            })
            print(f"Query {qnum:02d} done: {runtime:.3f}s rows={rows} cpu={metrics['cpu_percent']:.1f}% mem={metrics['peak_rss_mb']:.1f}MB r={metrics['disk_read_bytes']} w={metrics['disk_write_bytes']}")
        except Exception as e:
            # ensure monitor stopped
            try:
                monitor.stop()
            except Exception:
                pass
            # Attempt to get metrics even on failure, if possible
            try:
                metrics = monitor.get_metrics()
                cpu_p = metrics["cpu_percent"]
                peak_m = metrics["peak_rss_mb"]
                r_bytes = metrics["disk_read_bytes"]
                w_bytes = metrics["disk_write_bytes"]
            except Exception:
                cpu_p = peak_m = r_bytes = w_bytes = None
            runtime = time.time() - monitor.start_time if getattr(monitor, "start_time", None) else None
            records.append({
                "query_id": qnum,
                "status": "error",
                "runtime_seconds": runtime,
                "result_row_count": None,
                "cpu_percent": cpu_p,
                "peak_rss_mb": peak_m,
                "disk_read_bytes": r_bytes,
                "disk_write_bytes": w_bytes,
                "error": str(e)
            })
            print(f"Query {qnum:02d} failed: {e}")
    return pd.DataFrame(records)

# Plot helpers
def plot_bar(df, x_col, y_col, title, ylabel, outpath, color="tab:blue", logy=False):
    plt.figure(figsize=(14, 5))
    vals = df[y_col].fillna(0)
    plt.bar(df[x_col], vals, color=color)
    plt.xlabel("TPC-DS Query")
    plt.ylabel(ylabel)
    plt.title(title)
    if logy:
        plt.yscale("log")
    plt.grid(axis="y", linestyle=":", alpha=0.6)
    plt.xticks(range(1, 100, 2))
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()

def plot_disk_io(df, outpath):
    ids = df["query_id"]
    read_b = df["disk_read_bytes"].fillna(0)
    write_b = df["disk_write_bytes"].fillna(0)

    plt.figure(figsize=(14, 5))
    plt.bar(ids, read_b, color="tab:cyan", label="read bytes")
    plt.bar(ids, write_b, bottom=read_b, color="tab:olive", label="write bytes")
    plt.xlabel("TPC-DS Query")
    plt.ylabel("Disk I/O bytes (system-wide delta)")
    plt.title("Disk Read/Write Bytes per Query")
    plt.legend()
    plt.grid(axis="y", linestyle=":", alpha=0.6)
    plt.xticks(range(1, 100, 2))
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()

def plot_combined(qdf, outpath):
    # Combined: runtime (left y) and result row counts (right y)
    ids = qdf["query_id"]
    runtimes = qdf["runtime_seconds"].fillna(0)
    row_counts = qdf["result_row_count"].fillna(0)

    fig, ax1 = plt.subplots(figsize=(16, 6))

    color_rt = "tab:blue"
    ax1.set_xlabel("TPC-DS Query")
    ax1.set_ylabel("Runtime (seconds)", color=color_rt)
    ax1.bar(ids - 0.2, runtimes, width=0.4, color=color_rt, label="Runtime (s)")
    ax1.tick_params(axis="y", labelcolor=color_rt)
    ax1.set_xticks(range(1, 100, 2))

    ax2 = ax1.twinx()
    color_rc = "tab:orange"
    ax2.set_ylabel("Result row count", color=color_rc)
    ax2.bar(ids + 0.2, row_counts, width=0.4, color=color_rc, label="Result rows")
    ax2.tick_params(axis="y", labelcolor=color_rc)

    handles1, labels1 = ax1.get_legend_handles_labels()
    handles2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(handles1 + handles2, labels1 + labels2, loc="upper right")

    plt.title("TPC-DS Query Runtime and Result Row Counts (queries 1..99)")
    fig.tight_layout()
    plt.grid(axis="y", linestyle=":", alpha=0.2)
    plt.savefig(outpath, dpi=150)
    plt.close()

if __name__ == "__main__":
    queries = load_queries_from_dir(QUERIES_DIR)
    if not queries:
        print(f"No SQL files found in {QUERIES_DIR}. Place q01.sql..q99.sql files there and re-run.")

    con = duckdb.connect(database=DB_PATH, read_only=False)

    # table counts
    tables = list_tables(con)
    print(f"Found {len(tables)} tables in database.")
    table_counts_df = get_table_row_counts(con, tables)
    table_counts_df.to_csv(OUTPUT_DIR / "tpcds_table_row_counts.csv", index=False)

    # run queries with profiling
    query_summary_df = execute_and_profile_queries(con, queries)
    query_summary_df.to_csv(OUTPUT_DIR / "tpcds_query_summary_with_metrics.csv", index=False)

    qdf = query_summary_df.sort_values("query_id").reset_index(drop=True)

    # Plots
    plot_bar(qdf, "query_id", "runtime_seconds",
             "TPC-DS Query Runtimes (queries 1..99)", "Runtime (seconds)",
             OUTPUT_DIR / "tpcds_query_runtimes.png", color="tab:blue")

    plot_bar(qdf, "query_id", "result_row_count",
             "TPC-DS Query Result Row Counts (queries 1..99)", "Result row count",
             OUTPUT_DIR / "tpcds_query_row_counts.png", color="tab:green", logy=True)

    plot_bar(qdf, "query_id", "cpu_percent",
             "CPU Utilization per Query", "CPU utilization percent (process avg over wall time)",
             OUTPUT_DIR / "tpcds_query_cpu_percent.png", color="tab:red")

    plot_bar(qdf, "query_id", "peak_rss_mb",
             "Peak Process Memory (RSS) per Query", "Peak RSS (MB)",
             OUTPUT_DIR / "tpcds_query_peak_rss_mb.png", color="tab:purple", logy=False)

    plot_disk_io(qdf, OUTPUT_DIR / "tpcds_query_disk_io.png")

    plot_combined(qdf, OUTPUT_DIR / "tpcds_query_combined_runtime_rows.png")

    print("Done. Summary CSV and plots saved in", OUTPUT_DIR.resolve())
