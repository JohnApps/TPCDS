# cop_tpcds_test.py
# Generated by Moonshot Kimi
#
#!/usr/bin/env python3
# cop_tpcds_enhanced.py
# Enhanced TPC-DS Benchmark for DuckDB with proper query timeout
#!/usr/bin/env python3
# cop_tpcds_enhanced.py
# Enhanced TPC-DS Benchmark for DuckDB with proper query timeout
#!/usr/bin/env python3
# cop_tpcds_enhanced.py
# Enhanced TPC-DS Benchmark for DuckDB with proper API usage

#!/usr/bin/env python3
# cop_tpcds_enhanced.py
# Enhanced TPC-DS Benchmark for DuckDB with proper definition ordering
#!/usr/bin/env python3
# cop_tpcds_enhanced.py
# Enhanced TPC-DS Benchmark for DuckDB with query plan toggle

import argparse
import configparser
import gc
import math
import os
import re
import sys
import threading
import time
import traceback
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union

import duckdb
import matplotlib.pyplot as plt
import pandas as pd
import psutil

# ============================================================================
# TIMEOUT HANDLER
# ============================================================================

class QueryTimeoutError(Exception):
    """Raised when a query exceeds its time limit."""
    pass

class TimeoutManager:
    """Manages query execution with timeout using DuckDB's interrupt."""
    
    def __init__(self, timeout_seconds: int):
        self.timeout = timeout_seconds
        self._timer: Optional[threading.Timer] = None
        self._connection: Optional[duckdb.DuckDBPyConnection] = None
        self._lock = threading.Lock()
    
    def _timeout_handler(self):
        """Called when timeout expires."""
        with self._lock:
            if self._connection is not None:
                try:
                    self._connection.interrupt()
                except Exception:
                    pass
    
    def start(self, connection: duckdb.DuckDBPyConnection):
        """Start the timeout timer."""
        self._connection = connection
        if self.timeout > 0:
            self._timer = threading.Timer(self.timeout, self._timeout_handler)
            self._timer.start()
    
    def cancel(self):
        """Cancel the timeout and clean up."""
        with self._lock:
            if self._timer:
                self._timer.cancel()
                self._timer = None
            self._connection = None
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cancel()

# ============================================================================
# DATA STRUCTURES (DEFINED EARLY TO AVOID NameError)
# ============================================================================

@dataclass
class BenchmarkConfig:
    """Configuration container with type hints."""
    db_path: str = "tpcds_local.db"
    queries_dir: str = "tpcds_queries"
    output_dir: str = "tpcds_results"
    sample_interval: float = 0.1
    query_timeout_seconds: int = 300
    warmup_enabled: bool = True
    min_query_runtime_for_monitoring: float = 0.1
    query_range: str = "1-99"
    rerun_failed: bool = False
    export_query_plans: bool = True
    enable_gc: bool = True
    plots_dpi: int = 150
    create_download_package: bool = False

@dataclass
class QueryMetrics:
    """Metrics collected during query execution."""
    query_id: int
    status: str
    runtime_seconds: Optional[float]
    result_row_count: Optional[int]
    cpu_percent: Optional[float]
    peak_rss_mb: Optional[float]
    disk_read_bytes: Optional[int]
    disk_write_bytes: Optional[int]
    memory_growth_mb: Optional[float]
    error: Optional[str]
    error_code: Optional[str]
    stack_trace: Optional[str]
    query_plan: Optional[str] = field(default=None, repr=False)

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def parse_query_range(range_str: str) -> List[int]:
    """Parse query range string like '1,5,10-20,99' into list of IDs."""
    if not range_str:
        return list(range(1, 100))
    
    result = set()
    for part in range_str.split(','):
        part = part.strip()
        if '-' in part:
            start, end = map(int, part.split('-'))
            result.update(range(start, end + 1))
        else:
            result.add(int(part))
    return sorted(list(result))

def validate_inputs(config: BenchmarkConfig) -> None:
    """Validate all required inputs exist."""
    db_path = Path(config.db_path)
    if not db_path.exists():
        raise FileNotFoundError(f"Database not found: {db_path.resolve()}")
    
    queries_dir = Path(config.queries_dir)
    if not queries_dir.exists():
        raise FileNotFoundError(f"Queries directory not found: {queries_dir.resolve()}")
    
    query_files = list(queries_dir.glob("*.sql"))
    if not query_files:
        raise ValueError(f"No SQL files found in {queries_dir.resolve()}")
    
    print(f"‚úÖ Validation passed: {len(query_files)} queries found, {db_path.stat().st_size // 1024**2}MB database")

def geomean(arr: List[Union[int, float]]) -> float:
    """Calculate geometric mean of positive numbers."""
    if not arr:
        return 0.0
    valid = [float(x) for x in arr if x is not None and x > 0]
    if not valid:
        return 0.0
    log_sum = sum(math.log(x) for x in valid)
    return math.exp(log_sum / len(valid))

def create_default_files():
    """Create default configuration files if they don't exist."""
    if not Path("config.ini").exists():
        Path("config.ini").write_text(DEFAULT_CONFIG_INI)
        print("üìÑ Created default config.ini")
    
    if not Path("requirements.txt").exists():
        Path("requirements.txt").write_text(DEFAULT_REQUIREMENTS_TXT)
        print("üìÑ Created default requirements.txt")
    
    if not Path("README.md").exists():
        Path("README.md").write_text(DEFAULT_README_MD)
        print("üìÑ Created default README.md")

def create_download_package():
    """Create a zip file containing all files for easy distribution."""
    import zipfile
    
    package_name = "tpcds_benchmark_package.zip"
    files_to_package = [
        __file__,
        "config.ini",
        "requirements.txt",
        "README.md"
    ]
    
    with zipfile.ZipFile(package_name, 'w') as zipf:
        for file in files_to_package:
            if Path(file).exists():
                zipf.write(file)
            else:
                # Embed default content
                if file == "config.ini":
                    zipf.writestr(file, DEFAULT_CONFIG_INI)
                elif file == "requirements.txt":
                    zipf.writestr(file, DEFAULT_REQUIREMENTS_TXT)
                elif file == "README.md":
                    zipf.writestr(file, DEFAULT_README_MD)
    
    print(f"\nüì¶ Download package created: {Path(package_name).resolve()}")
    return package_name

# ============================================================================
# RESOURCE MONITORING
# ============================================================================

class ResourceMonitor:
    """Enhanced background resource monitor with per-process I/O and memory growth tracking."""
    
    def __init__(self, interval: float = 0.1):
        self.interval = interval
        self._stop_event = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self.proc = psutil.Process(os.getpid())
        self.num_cpus = psutil.cpu_count(logical=True) or 1
        
        # Baseline snapshots
        self.disk_before = None
        self.cpu_times_before = None
        self.mem_before: int = 0
        self.start_time: float = 0.0
        
        # Samples
        self.rss_samples: List[int] = []
        self.timestamps: List[float] = []
        self.disk_samples: List[Tuple[int, int]] = []
        self._lock = threading.Lock()
    
    def _sample_loop(self) -> None:
        """Background sampling thread."""
        while not self._stop_event.is_set():
            with self._lock:
                ts = time.time()
                try:
                    rss = self.proc.memory_info().rss
                except Exception:
                    rss = 0
                try:
                    # Per-process I/O counters (more accurate than system-wide)
                    io = self.proc.io_counters()
                    read_b = getattr(io, "read_bytes", 0)
                    write_b = getattr(io, "write_bytes", 0)
                except Exception:
                    read_b = write_b = 0
                
                self.rss_samples.append(rss)
                self.disk_samples.append((read_b, write_b))
                self.timestamps.append(ts)
            time.sleep(self.interval)
    
    def start(self) -> None:
        """Initialize monitoring and start background thread."""
        self.start_time = time.time()
        self.disk_before = self.proc.io_counters()
        self.cpu_times_before = self.proc.cpu_times()
        self.mem_before = self.proc.memory_info().rss
        
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._sample_loop, daemon=True)
        self._thread.start()
    
    def stop(self) -> None:
        """Stop monitoring and gather final metrics."""
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=1.0)
        self.end_time = time.time()
        self.disk_after = self.proc.io_counters()
        self.cpu_times_after = self.proc.cpu_times()
        self.mem_after = self.proc.memory_info().rss
    
    def get_metrics(self) -> Dict[str, float]:
        """Calculate and return performance metrics."""
        wall_time = max(1e-9, (self.end_time - self.start_time))
        
        # CPU utilization (normalized to number of CPUs)
        cpu_delta = (
            (self.cpu_times_after.user + self.cpu_times_after.system) -
            (self.cpu_times_before.user + self.cpu_times_before.system)
        )
        cpu_util_percent = (cpu_delta / wall_time) * 100.0 / (self.num_cpus or 1)
        
        # Peak RSS
        with self._lock:
            peak_rss = max(self.rss_samples) if self.rss_samples else 0
        
        # Memory growth
        mem_growth = (self.mem_after - self.mem_before) / (1024.0 ** 2)
        
        # Disk I/O deltas (per-process)
        read_bytes = max(0, self.disk_after.read_bytes - self.disk_before.read_bytes)
        write_bytes = max(0, self.disk_after.write_bytes - self.disk_before.write_bytes)
        
        return {
            "cpu_percent": cpu_util_percent,
            "peak_rss_mb": peak_rss / (1024.0 ** 2),
            "memory_growth_mb": mem_growth,
            "disk_read_bytes": read_bytes,
            "disk_write_bytes": write_bytes,
            "wall_time": wall_time
        }

# ============================================================================
# DATABASE FUNCTIONS
# ============================================================================

def load_queries_from_dir(dirpath: Path) -> Dict[int, str]:
    """Load TPC-DS queries from directory.
    
    Args:
        dirpath: Directory containing q*.sql files
        
    Returns:
        Dictionary mapping query number to SQL string
    """
    pattern = re.compile(r"q?0*([1-9][0-9]?)\.sql$", re.IGNORECASE)
    queries = {}
    
    if not dirpath.exists():
        return queries
    
    for p in sorted(dirpath.glob("*.sql")):
        m = pattern.search(p.name)
        if m:
            qnum = int(m.group(1))
            sql = p.read_text(encoding="utf8")
            queries[qnum] = sql
    
    return queries

def list_tables(connection: duckdb.DuckDBPyConnection) -> List[str]:
    """Get list of user tables in database."""
    q = """
    SELECT table_schema, table_name
    FROM information_schema.tables
    WHERE table_type = 'BASE TABLE'
      AND table_schema NOT IN ('pg_catalog', 'information_schema')
    ORDER BY table_schema, table_name
    """
    df = connection.execute(q).fetchdf()
    full_names = []
    for _, row in df.iterrows():
        schema, tbl = row["table_schema"], row["table_name"]
        if schema and schema != "main":
            full_names.append(f"{schema}.{tbl}")
        else:
            full_names.append(tbl)
    return full_names

def get_table_row_counts(connection: duckdb.DuckDBPyConnection, 
                        tables: List[str]) -> pd.DataFrame:
    """Get row counts for all tables."""
    table_counts = []
    for tbl in tables:
        try:
            cnt = connection.execute(f"SELECT COUNT(*) AS c FROM {tbl}").fetchone()[0]
        except Exception as e:
            cnt = None
            print(f"‚ö†Ô∏è  Warning: counting rows for {tbl} failed: {e}")
        table_counts.append({"table": tbl, "row_count": cnt})
    return pd.DataFrame(table_counts)

def export_query_plan(connection: duckdb.DuckDBPyConnection, 
                     sql: str, qnum: int, output_dir: Path) -> Optional[str]:
    """Export query execution plan to file."""
    try:
        plan_dir = output_dir / "query_plans"
        plan_dir.mkdir(exist_ok=True)
        plan = connection.execute(f"EXPLAIN ANALYZE {sql}").fetchdf()
        plan_path = plan_dir / f"q{qnum:02d}_plan.csv"
        plan.to_csv(plan_path, index=False)
        return str(plan_path)
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not export plan for Q{qnum}: {e}")
        return None

# ============================================================================
# QUERY EXECUTION WITH CORRECTED API
# ============================================================================

def execute_and_profile_queries(connection: duckdb.DuckDBPyConnection,
                               queries_dict: Dict[int, str],
                               config: BenchmarkConfig,
                               failed_queries: set) -> List[QueryMetrics]:
    """Execute queries with comprehensive profiling and proper timeout handling."""
    metrics_list = []
    query_ids = parse_query_range(config.query_range)
    
    # Filter for rerun_failed mode
    if config.rerun_failed and failed_queries:
        query_ids = [qid for qid in query_ids if qid in failed_queries]
        print(f"üîÑ Rerun mode: executing {len(query_ids)} previously failed queries")
    
    for qnum in query_ids:
        if qnum not in queries_dict:
            metrics_list.append(QueryMetrics(
                query_id=qnum,
                status="missing",
                runtime_seconds=None,
                result_row_count=None,
                cpu_percent=None,
                peak_rss_mb=None,
                disk_read_bytes=None,
                disk_write_bytes=None,
                memory_growth_mb=None,
                error="SQL file missing",
                error_code="FILE_NOT_FOUND"
            ))
            print(f"‚ùå Query {qnum:02d}: SQL file missing")
            continue
        
        sql = queries_dict[qnum]
        
        # Warm-up run - PROPER DUCKDB API
        if config.warmup_enabled:
            try:
                # Execute but don't store results
                connection.execute(sql)
                # Consume all results to ensure complete execution
                while True:
                    chunk = connection.fetch_df_chunk()
                    if chunk is None:
                        break
                if config.enable_gc:
                    gc.collect()
                print(f"üî• Query {qnum:02d}: Warm-up completed")
            except Exception as e:
                print(f"‚ö†Ô∏è  Query {qnum:02d}: Warm-up failed: {e}")
        
        # Export query plan (conditionally)
        plan_path = None
        if config.export_query_plans:
            plan_path = export_query_plan(connection, sql, qnum, Path(config.output_dir))
        
        # Execute with monitoring and timeout
        monitor = ResourceMonitor(interval=config.sample_interval)
        
        try:
            with TimeoutManager(config.query_timeout_seconds) as timeout_mgr:
                monitor.start()
                t0 = time.time()
                
                # Execute query
                connection.execute(sql)
                timeout_mgr.start(connection)  # Start timeout after execute
                
                # Count rows using chunked fetching
                rows = 0
                try:
                    while True:
                        chunk = connection.fetch_df_chunk()
                        if chunk is None:
                            break
                        rows += len(chunk)
                except AttributeError:
                    # Fallback for older DuckDB versions
                    print(f"‚ö†Ô∏è  fetch_df_chunk() not available, falling back to fetchdf()")
                    df_result = connection.fetchdf()
                    rows = len(df_result)
                    del df_result
                    if config.enable_gc:
                        gc.collect()
                
                runtime = time.time() - t0
                
                # Force garbage collection
                if config.enable_gc:
                    gc.collect()
                
                monitor.stop()
                metrics = monitor.get_metrics()
                
                metrics_list.append(QueryMetrics(
                    query_id=qnum,
                    status="ok",
                    runtime_seconds=runtime,
                    result_row_count=rows,
                    cpu_percent=metrics["cpu_percent"],
                    peak_rss_mb=metrics["peak_rss_mb"],
                    disk_read_bytes=metrics["disk_read_bytes"],
                    disk_write_bytes=metrics["disk_write_bytes"],
                    memory_growth_mb=metrics["memory_growth_mb"],
                    error=None,
                    error_code=None,
                    stack_trace=None,
                    query_plan=plan_path
                ))
                
                print(f"‚úÖ Query {qnum:02d}: {runtime:.3f}s, rows={rows:,}, "
                      f"cpu={metrics['cpu_percent']:.1f}%, mem={metrics['peak_rss_mb']:.1f}MB, "
                      f"io=({metrics['disk_read_bytes']:,}r, {metrics['disk_write_bytes']:,}w)")
            
        except QueryTimeoutError as e:
            monitor.stop()
            runtime = time.time() - t0 if 't0' in locals() else None
            metrics_list.append(QueryMetrics(
                query_id=qnum,
                status="timeout",
                runtime_seconds=runtime,
                result_row_count=None,
                cpu_percent=None,
                peak_rss_mb=None,
                disk_read_bytes=None,
                disk_write_bytes=None,
                memory_growth_mb=None,
                error=str(e),
                error_code="TIMEOUT",
                stack_trace=None,
                query_plan=plan_path
            ))
            print(f"‚è±Ô∏è  Query {qnum:02d}: TIMEOUT after {runtime:.1f}s")
            
        except Exception as e:
            # For other errors, still try to get metrics
            try:
                monitor.stop()
                metrics = monitor.get_metrics()
                runtime = metrics["wall_time"]
                cpu_p = metrics["cpu_percent"]
                peak_m = metrics["peak_rss_mb"]
                r_bytes = metrics["disk_read_bytes"]
                w_bytes = metrics["disk_write_bytes"]
                mem_growth = metrics["memory_growth_mb"]
            except Exception:
                runtime = cpu_p = peak_m = r_bytes = w_bytes = mem_growth = None
            
            metrics_list.append(QueryMetrics(
                query_id=qnum,
                status="error",
                runtime_seconds=runtime,
                result_row_count=None,
                cpu_percent=cpu_p,
                peak_rss_mb=peak_m,
                disk_read_bytes=r_bytes,
                disk_write_bytes=w_bytes,
                memory_growth_mb=mem_growth,
                error=str(e),
                error_code=type(e).__name__,
                stack_trace=traceback.format_exc(),
                query_plan=plan_path
            ))
            
            print(f"‚ùå Query {qnum:02d}: FAILED - {e}")
        
        finally:
            # Ensure cleanup
            try:
                monitor.stop()
            except Exception:
                pass
    
    return metrics_list

# ============================================================================
# PLOTTING FUNCTIONS
# ============================================================================

def plot_bar(df: pd.DataFrame, x_col: str, y_col: str, title: str, ylabel: str,
            outpath: Path, color: str = "tab:blue", logy: bool = False,
            dpi: int = 150) -> None:
    """Create a bar plot with standardized formatting."""
    plt.figure(figsize=(14, 5))
    vals = df[y_col].fillna(0)
    plt.bar(df[x_col], vals, color=color)
    plt.xlabel("TPC-DS Query")
    plt.ylabel(ylabel)
    plt.title(title)
    if logy:
        plt.yscale("log")
    plt.grid(axis="y", linestyle=":", alpha=0.6)
    plt.xticks(range(1, 100, 2))
    plt.tight_layout()
    plt.savefig(outpath, dpi=dpi)
    plt.close()

def plot_disk_io(df: pd.DataFrame, outpath: Path, dpi: int = 150) -> None:
    """Create stacked bar plot for disk I/O."""
    ids = df["query_id"]
    read_b = df["disk_read_bytes"].fillna(0)
    write_b = df["disk_write_bytes"].fillna(0)

    plt.figure(figsize=(14, 5))
    plt.bar(ids, read_b, color="tab:cyan", label="read bytes")
    plt.bar(ids, write_b, bottom=read_b, color="tab:olive", label="write bytes")
    plt.xlabel("TPC-DS Query")
    plt.ylabel("Disk I/O bytes (per-process)")
    plt.title("Disk Read/Write Bytes per Query")
    plt.legend()
    plt.grid(axis="y", linestyle=":", alpha=0.6)
    plt.xticks(range(1, 100, 2))
    plt.tight_layout()
    plt.savefig(outpath, dpi=dpi)
    plt.close()

def plot_combined(df: pd.DataFrame, outpath: Path, dpi: int = 150) -> None:
    """Create combined plot with runtime and row counts on dual axes."""
    ids = df["query_id"]
    runtimes = df["runtime_seconds"].fillna(0)
    row_counts = df["result_row_count"].fillna(0)

    fig, ax1 = plt.subplots(figsize=(16, 6))

    color_rt = "tab:blue"
    ax1.set_xlabel("TPC-DS Query")
    ax1.set_ylabel("Runtime (seconds)", color=color_rt)
    ax1.bar(ids - 0.2, runtimes, width=0.4, color=color_rt, label="Runtime (s)")
    ax1.tick_params(axis="y", labelcolor=color_rt)
    ax1.set_xticks(range(1, 100, 2))

    ax2 = ax1.twinx()
    color_rc = "tab:orange"
    ax2.set_ylabel("Result row count", color=color_rc)
    ax2.bar(ids + 0.2, row_counts, width=0.4, color=color_rc, label="Result rows")
    ax2.tick_params(axis="y", labelcolor=color_rc)

    handles1, labels1 = ax1.get_legend_handles_labels()
    handles2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(handles1 + handles2, labels1 + labels2, loc="upper right")

    plt.title("TPC-DS Query Runtime and Result Row Counts")
    fig.tight_layout()
    plt.grid(axis="y", linestyle=":", alpha=0.2)
    plt.savefig(outpath, dpi=dpi)
    plt.close()

def generate_all_plots(df: pd.DataFrame, output_dir: Path, dpi: int) -> None:
    """Generate all plots using configuration loop."""
    plot_configs = [
        ("runtime_seconds", "Runtime (s)", "tab:blue", False),
        ("result_row_count", "Row Count", "tab:green", True),
        ("cpu_percent", "CPU %", "tab:red", False),
        ("peak_rss_mb", "Peak RSS (MB)", "tab:purple", False),
        ("memory_growth_mb", "Memory Growth (MB)", "tab:brown", False),
    ]
    
    for metric, label, color, logy in plot_configs:
        plot_bar(df, "query_id", metric, f"TPC-DS {label} per Query", label,
                output_dir / f"tpcds_query_{metric}.png", color, logy, dpi)
    
    plot_disk_io(df, output_dir / "tpcds_query_disk_io.png", dpi)
    plot_combined(df, output_dir / "tpcds_query_combined_runtime_rows.png", dpi)

# ============================================================================
# MAIN EXECUTION
# ============================================================================

DEFAULT_CONFIG_INI = """[BENCHMARK]
db_path = tpcds_local.db
queries_dir = tpcds_queries
output_dir = tpcds_results
sample_interval = 0.1
query_timeout_seconds = 300
warmup_enabled = true
min_query_runtime_for_monitoring = 0.1

[EXECUTION]
query_range = 1-99
rerun_failed = false
export_query_plans = true
enable_gc = true

[OUTPUT]
plots_dpi = 150
create_download_package = false
"""

DEFAULT_REQUIREMENTS_TXT = """duckdb>=0.9.0
pandas>=2.0.0
matplotlib>=3.7.0
psutil>=5.9.0
"""

DEFAULT_README_MD = """# TPC-DS Benchmark for DuckDB

This package contains an enhanced TPC-DS benchmark suite for DuckDB with query timeout support.

## Quick Start

1. Install dependencies: `pip install -r requirements.txt`
2. Configure: Edit `config.ini` or use command-line arguments
3. Prepare data: Ensure `tpcds_local.db` exists with TPC-DS schema
4. Place queries: Put q01.sql..q99.sql in the `tpcds_queries/` directory
5. Run: `python cop_tpcds_enhanced.py`

## Configuration

Edit `config.ini` or use CLI args:
- `--db-path`: Path to DuckDB database
- `--queries-dir`: Directory containing SQL files
- `--query-range`: Range to execute (e.g., "1-50" or "1,5,10-20")
- `--sample-interval`: Resource monitoring frequency
- `--query-timeout`: Per-query timeout in seconds (0 to disable)
- `--warmup`: Enable warm-up runs
- `--no-warmup`: Disable warm-up runs
- `--export-plans`: Export query execution plans (default: enabled)
- `--no-export-plans`: Disable query plan export
- `--package`: Create download package after run

## Output

Results are saved to `tpcds_results/`:
- `tpcds_query_summary_with_metrics.csv`: Full performance metrics
- `tpcds_table_row_counts.csv`: Table statistics
- `tpcds_benchmark_summary.csv`: Statistical summary
- `query_plans/`: Execution plans (if enabled)
- `*.png`: Performance visualizations
- `benchmark_package.zip`: Self-contained download package (if --package used)

## Timeout Mechanism

Since DuckDB doesn't natively support statement timeouts, this benchmark uses Python's threading to interrupt queries that exceed the time limit. Set `--query-timeout 0` to disable.

## API Compatibility

This script uses `fetch_df_chunk()` which requires DuckDB >= 0.7.0. For older versions, it will fall back to `fetchdf()`.
"""

def main():
    """Main benchmark execution flow."""
    # Create default files if missing
    create_default_files()
    
    # Parse CLI arguments
    parser = argparse.ArgumentParser(
        description="Enhanced TPC-DS Benchmark for DuckDB with Query Timeout",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--config", "-c", help="Configuration file path", default="config.ini")
    parser.add_argument("--db-path", help="Path to DuckDB database")
    parser.add_argument("--queries-dir", help="Directory containing SQL files")
    parser.add_argument("--output-dir", help="Directory for results")
    parser.add_argument("--query-range", help="Range: '1-99' or '1,5,10-20'")
    parser.add_argument("--sample-interval", type=float, help="Monitoring interval in seconds")
    parser.add_argument("--query-timeout", type=int, help="Per-query timeout in seconds (0 to disable)")
    parser.add_argument("--warmup", action="store_true", help="Enable warm-up runs")
    parser.add_argument("--no-warmup", action="store_true", help="Disable warm-up runs")
    parser.add_argument("--export-plans", action="store_true", help="Export query execution plans")
    parser.add_argument("--no-export-plans", action="store_true", help="Disable query plan export")
    parser.add_argument("--rerun-failed", action="store_true", help="Rerun previously failed queries")
    parser.add_argument("--package", action="store_true", help="Create download package after run")
    parser.add_argument("--validate-only", action="store_true", help="Only validate setup, don't run")
    
    args = parser.parse_args()
    
    # Load configuration (CLI args override config file)
    config = BenchmarkConfig()
    if Path(args.config).exists():
        cfg = configparser.ConfigParser()
        cfg.read(args.config)
        if "BENCHMARK" in cfg:
            config.db_path = cfg["BENCHMARK"].get("db_path", config.db_path)
            config.queries_dir = cfg["BENCHMARK"].get("queries_dir", config.queries_dir)
            config.output_dir = cfg["BENCHMARK"].get("output_dir", config.output_dir)
            config.sample_interval = cfg["BENCHMARK"].getfloat("sample_interval", config.sample_interval)
            config.query_timeout_seconds = cfg["BENCHMARK"].getint("query_timeout_seconds", config.query_timeout_seconds)
            config.warmup_enabled = cfg["BENCHMARK"].getboolean("warmup_enabled", config.warmup_enabled)
            config.min_query_runtime_for_monitoring = cfg["BENCHMARK"].getfloat("min_query_runtime_for_monitoring", config.min_query_runtime_for_monitoring)
        if "EXECUTION" in cfg:
            config.query_range = cfg["EXECUTION"].get("query_range", config.query_range)
            config.rerun_failed = cfg["EXECUTION"].getboolean("rerun_failed", config.rerun_failed)
            config.export_query_plans = cfg["EXECUTION"].getboolean("export_query_plans", config.export_query_plans)
            config.enable_gc = cfg["EXECUTION"].getboolean("enable_gc", config.enable_gc)
        if "OUTPUT" in cfg:
            config.plots_dpi = cfg["OUTPUT"].getint("plots_dpi", config.plots_dpi)
            config.create_download_package = cfg["OUTPUT"].getboolean("create_download_package", config.create_download_package)
    
    # CLI overrides
    if args.db_path: config.db_path = args.db_path
    if args.queries_dir: config.queries_dir = args.queries_dir
    if args.output_dir: config.output_dir = args.output_dir
    if args.query_range: config.query_range = args.query_range
    if args.sample_interval: config.sample_interval = args.sample_interval
    if args.query_timeout is not None: config.query_timeout_seconds = args.query_timeout
    if args.warmup: config.warmup_enabled = True
    if args.no_warmup: config.warmup_enabled = False
    if args.export_plans: config.export_query_plans = True
    if args.no_export_plans: config.export_query_plans = False
    if args.rerun_failed: config.rerun_failed = True
    if args.package: config.create_download_package = True
    
    print("=" * 70)
    print("üöÄ TPC-DS Enhanced Benchmark")
    print("=" * 70)
    print(f"Database: {config.db_path}")
    print(f"Queries: {config.queries_dir}")
    print(f"Output: {config.output_dir}")
    print(f"Range: {config.query_range}")
    print(f"Warmup: {config.warmup_enabled}")
    print(f"Export Plans: {config.export_query_plans}")
    print(f"Timeout: {config.query_timeout_seconds}s")
    print("=" * 70)
    
    # Validate inputs
    try:
        validate_inputs(config)
    except Exception as e:
        print(f"‚ùå Validation failed: {e}")
        sys.exit(1)
    
    if args.validate_only:
        print("‚úÖ Setup validation complete. Exiting.")
        sys.exit(0)
    
    # Create output directory
    output_dir = Path(config.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Load queries
    queries_dir = Path(config.queries_dir)
    queries = load_queries_from_dir(queries_dir)
    print(f"üìÇ Loaded {len(queries)} queries from {queries_dir}")
    
    # Check for previous failures if in rerun mode
    failed_queries = set()
    summary_csv = output_dir / "tpcds_query_summary_with_metrics.csv"
    if config.rerun_failed and summary_csv.exists():
        try:
            prev_df = pd.read_csv(summary_csv)
            failed_queries = set(prev_df[prev_df['status'] == 'error']['query_id'].tolist())
            print(f"üìã Found {len(failed_queries)} previously failed queries")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not read previous results: {e}")
    
    # Run benchmark
    with duckdb.connect(database=config.db_path, read_only=False) as con:
        # Table statistics
        tables = list_tables(con)
        print(f"üìä Found {len(tables)} tables")
        table_counts_df = get_table_row_counts(con, tables)
        table_counts_df.to_csv(output_dir / "tpcds_table_row_counts.csv", index=False)
        
        # Execute queries
        metrics_list = execute_and_profile_queries(con, queries, config, failed_queries)
        
        # Convert to DataFrame
        df = pd.DataFrame([vars(m) for m in metrics_list])
        df = df.sort_values("query_id").reset_index(drop=True)
        
        # Save detailed results
        df.to_csv(summary_csv, index=False)
        
        # Generate summary statistics
        successful_runs = df[df['status'] == 'ok']
        summary_stats = {
            'total_queries': len(df),
            'queries_successful': len(successful_runs),
            'queries_failed': len(df[df['status'] == 'error']),
            'queries_timeout': len(df[df['status'] == 'timeout']),
            'queries_missing': len(df[df['status'] == 'missing']),
            'total_wall_time_seconds': successful_runs['runtime_seconds'].sum(),
            'mean_runtime_seconds': successful_runs['runtime_seconds'].mean(),
            'median_runtime_seconds': successful_runs['runtime_seconds'].median(),
            'geomean_runtime_seconds': geomean(successful_runs['runtime_seconds'].tolist()),
            'mean_peak_rss_mb': successful_runs['peak_rss_mb'].mean(),
            'mean_cpu_percent': successful_runs['cpu_percent'].mean(),
            'total_rows_returned': successful_runs['result_row_count'].sum(),
            'benchmark_timestamp': pd.Timestamp.now().isoformat()
        }
        
        summary_df = pd.DataFrame([summary_stats])
        summary_df.to_csv(output_dir / "tpcds_benchmark_summary.csv", index=False)
        
        print("\n" + "=" * 70)
        print("üìà BENCHMARK SUMMARY")
        print("=" * 70)
        print(f"Successful queries: {summary_stats['queries_successful']}/{summary_stats['total_queries']}")
        print(f"Timeouts: {summary_stats['queries_timeout']}")
        print(f"Total time: {summary_stats['total_wall_time_seconds']:.2f}s")
        print(f"Geomean runtime: {summary_stats['geomean_runtime_seconds']:.3f}s")
        print(f"Mean memory: {summary_stats['mean_peak_rss_mb']:.1f}MB")
        print(f"Total rows: {summary_stats['total_rows_returned']:,}")
        print("=" * 70)
        
        # Generate plots
        if not successful_runs.empty:
            print("üìä Generating plots...")
            generate_all_plots(df, output_dir, config.plots_dpi)
        
        # Create download package if requested
        if config.create_download_package or args.package:
            package_path = create_download_package()
            print(f"üì¶ Package ready: {package_path}")
        
        print(f"\n‚úÖ Benchmark complete. Results saved to: {output_dir.resolve()}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚õî Benchmark interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        traceback.print_exc()
        sys.exit(1)